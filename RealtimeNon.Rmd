---
title: "R Notebook"
output: PDF
---
# Library

```{r, message = FALSE}
# null vs change existing performance comparison (100/200+)
library(fda)
library(gmfd)
library(doSNOW)
library(pracma) 
library(foreach)
library(fda.usc)
library(ggplot2)
library(gStream)
library(reshape2)
library(doParallel)
library(fdaoutlier)
```

# Simulate functional data

```{r}
t_values <- seq(0, 1, length.out = 1000)
n <- 200    # 這是要模擬的功能實現的數量
L <- 150    # 基擴展中的項數
sim1 <- function(rho = 0.2, mean_change=TRUE, scale, cov_change=FALSE){
  
  phi <- function(l, t) {
    if (l == 0) {
      return(1)
    } else if (l %% 2 == 1) {
      k <- (l + 1) / 2
      return(sqrt(2) * sin(2 * pi * k * t - pi))
    } else {
      k <- l / 2
      return(sqrt(2) * cos(2 * pi * k * t - pi))
    }
  }
  
  # plot(t_values, phi(1, t_values), type = "l", xlab = "t", ylab = "phi(150, t)")
  # for(i in 2:20) {
  #   lines(t_values, phi(i, t_values), col = "red")
  # }
  
  lambda <- function(l) {
    return(0.7 * 2^(-l))
  }
  generate_tau <- function(n, L, rho, scale, cov_change) {
    tau <- matrix(0, n, L + 1)
    for (i in 1:n) {
      for (l in 1:(L + 1)) {
        if (i == 1) {
          tau[i,l] <- rnorm(1)
        }
        else if (i <= 150){ # 3/4 * n
          tau[i,l] <- rho * tau[i - 1, l] + rnorm(1)
        }
        else{
          tau[i,l] <- rho * tau[i - 1, l] + scale * ifelse(cov_change, rexp(1), rnorm(1))
        }
      }
    }
    return(tau)
  }
  
  mean_function <- function(t, i, n) {
    if(mean_change == TRUE){
      if (i <= 150) { # 3/4 * n
        return(0.5 - 100 * (t - 0.1) * (t - 0.3) * (t - 0.5) * (t - 0.9) + 0.8 * sin(1+10*pi*t))
      } else{
        return(1 + 3*t^2 - 5*t^3)
      }
    }
    else{
      return(0.5 - 100 * (t - 0.1) * (t - 0.3) * (t - 0.5) * (t - 0.9) + 0.8 * sin(1+10*pi*t))
    }
  }
  # scale_function <- function(t, i, n) {
  #   if (i <= 3/4 * n) {
  #     return(t)
  #   } else {
  #     return(2*t)
  #   }
  # }
  simulate_Y <- function(n, L, rho, t_values) {
    Phi <- sapply(0:L, function(l) sapply(t_values, function(t) phi(l, t)))
    Lambda <- diag(sapply(0:L, function(l) sqrt(lambda(l))))
    Tau_scaled <- generate_tau(n, L, rho, scale, cov_change) %*% t(Lambda)
    MeanVals <- matrix(nrow = n, ncol = length(t_values))
    for (i in 1:n) {
      for (t_index in 1:length(t_values)) {
        MeanVals[i, t_index] <- mean_function(t_values[t_index], i, n)
      }
    }
    Y <- MeanVals + Tau_scaled %*% t(Phi)
    return(Y)
    
    # tau <- generate_tau(n, L, rho, scale)
    # Y <- matrix(0, n, length(t_values))
    # for (i in 1:n) {
    #   for (t_index in 1:length(t_values)) {
    #     t <- t_values[t_index]
    #     sum_val <- mean_function(t, i, n)
    #     for (l in 0:L) {
    #       sum_val <- sum_val + sqrt(lambda (l)) * tau[i,l + 1] * phi(l, t)
    #     }
    #     Y[i, t_index] <- sum_val
    #   }
    # }
    # return(Y)
  }
  Y_simulated <- simulate_Y(n, L, rho, t_values)
  
  #1. L2 distance
  # integral_distance <- function(vec1, vec2) {
  #   f <- function(x) {
  #     # 插值找到當前x值下vec1和vec2的值
  #     y1 <- approx(1:length(vec1), vec1, x)$y
  #     y2 <- approx(1:length(vec2), vec2, x)$y
  #     (y1 - y2)^2
  #   }
  #   integral <- integrate(f, lower = 1, upper = length(vec1))
  #   sqrt(integral$value)
  # }

  #2. with derivative information
  # integral_distance <- function(vec1, vec2, t = t_values) {
  #   diff_square <- (vec1 - vec2)^2
  # 
  #   # 計算導數
  #   d_vec1 <- diff(vec1) / diff(t)
  #   d_vec2 <- diff(vec2) / diff(t)
  #   # 因為diff()會減少一個長度，所以需要對時間t作相同的處理
  #   t_diff <- t[-length(t)]
  # 
  #   # 計算導數的差值平方
  #   diff_deriv_square <- (d_vec1 - d_vec2)^2
  # 
  #   integral1 <- trapz(t, diff_square)
  #   integral2 <- trapz(t_diff, diff_deriv_square)
  # 
  #   # 平方根
  #   sqrt(integral1 + integral2)
  # }
  
  #3. General Mahalanobis distance
  # integral_distance <- function(FD1, i, j, t = t_values, eigval, eigfunc) {
  #   x <- gmfd::funData( t, FD1$data[[1]][i, ] )
  #   y <- gmfd::funData( t, FD1$data[[1]][j, ] )
  #   gmfd::funDist( x, y, metric = "mahalanobis", p = 10^5, eigval, eigfunc )
  # }

  # no_cores <- detectCores(logical = FALSE)
  # cl <- makeCluster(no_cores)
  # registerDoParallel(cl)
  # n = 100
  # 
  # distance_matrix <- foreach(i = 1:n, .combine='cbind', .export = "t_values", .packages = 'pracma') %dopar% {
  #   column_i <- numeric(n)
  #   for (j in 1:n) {
  #     column_i[j] <- integral_distance(Y_simulated[i, ], Y_simulated[j, ], t_values)
  #   }
  #   column_i
  # }
  # 
  # stopCluster(cl)
  # distance_matrix <- t(distance_matrix)
  # diag(distance_matrix) = max(distance_matrix)+100
  
  # list(Y_simulated=Y_simulated, distance_matrix=distance_matrix)
  return(Y_simulated)
}
# Graph
start = Sys.time()
Y_simulated = sim1(rho=0.2, mean_change=FALSE, scale=2, cov_change=FALSE)

par(mar = c(3, 3, 2, 2))
plot(t_values, Y_simulated[1,], type = "l", ylim = c(min(Y_simulated), max(Y_simulated)), xlab = "t", ylab = "Y(t)")
for (i in 2:150) {
  lines(t_values, Y_simulated[i,], col = "black")
}
for (i in 151:N) {
  lines(t_values, Y_simulated[i,], col = "red")
}
legend("topleft", legend = c("Before", "After"), col = c("black", "red"), lwd = 2, lty = 1, bty = "n")

end = Sys.time()
print(paste0("took: ", round(as.numeric(difftime(time1 = end, time2 = start, units = "secs")), 3), " seconds"))
```

# Settings

```{r}
# we can make some alternative, arl can't be derived analytically, but can be implemented well through simulation
# functional boxplot
dist2 <- function(vec1, vec2, t = t_values) {
  diff_square <- (vec1 - vec2)^2
  d_vec1 <- diff(vec1) / diff(t)
  d_vec2 <- diff(vec2) / diff(t)
  t_diff <- t[-length(t)]
  diff_deriv_square <- (d_vec1 - d_vec2)^2
  integral1 <- trapz(t, diff_square)
  integral2 <- trapz(t_diff, diff_deriv_square)
  sqrt(integral1 + integral2)
}

n <- 200
Y_simulated = sim1(rho=0.2, mean_change=TRUE, scale=1, cov_change=FALSE)
# dist(Y_simulated[1:2, ])
# dist2(Y_simulated[1, ], Y_simulated[2, ])

baseline <- function(simulate_data){
  N = 75; M = 25; k = 4
  X_1 = simulate_data[1:M, ]
  X_2 = simulate_data[(M+1):N, ]
  baseline_distances <- numeric(N - M)
  for (i in 1:(N - M)) {
    datum_i <- X_2[i, ]
    tmp_dist <- numeric(M)
    for (j in 1:M) {
      datum_j <- X_1[j, ]
      dist_ij <- dist2(datum_i, datum_j, t_values)  # 歐幾里得距離
      tmp_dist[j] <- dist_ij
    }
    sort_dist <- sort(tmp_dist)
    sum_kNN <- sum(sort_dist[1:k])
    baseline_distances[i] <- sum_kNN
  }
  return(baseline_distances)
}
baseline_distances <- baseline(Y_simulated)

##############################################################

# use analytical results "datum_t <- simulate_data[sample(1:50, 1), ]" not reasonable
start.time <- Sys.time()

h <- seq(2.5, 6.5, by = 0.05)
avg_RL <- function(h, simulate_data, baseline_distances){
  alpha <- 0.2; N = 75; M = 25; k = 4
  no_trials <- 50
  X_1 = simulate_data[1:M, ]
  # sum_fap <- rep(0, length(h))
  cl <- makeCluster(detectCores(logical = FALSE)-1)
  registerDoParallel(cl)
  # for (nn in 1:no_trials) {
  sum_fap <- foreach(nn = 1:no_trials, .combine = '+', .packages = c("doParallel", "pracma"), .export = c("sim1", "dist2", "t_values")) %dopar% {
    alarm_flag <- rep(0, length(h))
    g <- 0
    t <- 1
    local_sum_fap <- rep(0, length(h)) # added
    while (alarm_flag[length(h)] == 0) {
      n = 1; L = 150
      datum_t <- sim1(rho=0.2, mean_change=FALSE, scale=1, cov_change=FALSE)[1,] # distribution of no change
      # datum_t <- simulate_data[sample(1:N, 1), ]
      tmp_dist <- rep(0, M)
      for (j in 1:M) {
        datum_j <- X_1[j, ]
        tmp_dist[j] <- dist2(datum_t, datum_j, t_values)
      }
      sort_dist <- sort(tmp_dist)
      sum_kNN <- sum(sort_dist[1:k])
      tail_prob <- sum(baseline_distances > sum_kNN) / (N - M)
      if (tail_prob == 0) { # modification for small-size datasets
        tail_prob <- 1 / (N - M)
      }
      g <- g + log(alpha / tail_prob)
      if (g < 0) {
        g <- 0
      }
      # sum_fap <- sum_fap + t * (g >= h) * (alarm_flag == 0)
      local_sum_fap <- local_sum_fap + t * (g >= h) * (alarm_flag == 0)
      alarm_flag <- alarm_flag + (g >= h) * (alarm_flag == 0)
      t <- t + 1
    }
    return(local_sum_fap)
  }
  stopCluster(cl)
  mean_fap <- sum_fap / no_trials
  # no_cores <- detectCores(logical = F)
  # cl <- makeCluster(no_cores)
  # registerDoParallel(cl)
  # sum_fap <- foreach(nn = 1:no_trials, .combine = '+') %dopar% {
  #   alarm_flag <- rep(0, length(h))
  #   g <- 0
  #   t <- 1
  #   while (alarm_flag[length(h)] == 0) {
  #     datum_t <- simulate_data[sample(1:50, 1), ]
  #     tmp_dist <- rep(0, num)
  #     for (j in 1:num) {
  #       datum_j <- X_1[j, ]
  #       tmp_dist[j] <- sqrt(sum((datum_t - datum_j)^2))
  #     }
  #     sort_dist <- sort(tmp_dist)
  #     sum_kNN <- sum(sort_dist[1:k])
  #     tail_prob <- sum(baseline_distances > sum_kNN) / (N - M)
  #     if (tail_prob == 0) {
  #       tail_prob <- 1 / (N - M)
  #     }
  #     g <- g + log(alpha / tail_prob)
  #     if (g < 0) {
  #       g <- 0
  #     }
  #     sum_fap_local <- t * (g >= h) * (alarm_flag == 0)
  #     alarm_flag <- alarm_flag + (g >= h) * (alarm_flag == 0)
  #     t <- t + 1
  #   }
  #   return(sum_fap_local)
  # }
  # stopCluster(cl)
  # mean_fap <- sum_fap / no_trials
  return(mean_fap)
}
mean_fap <- avg_RL(h, Y_simulated, baseline_distances)
h[which(mean_fap>500)[1]] # 3.27
log(500/10.1)/(1-lambertWp(0.2*log(0.2))/log(0.2))

end.time <- Sys.time()
print(end.time - start.time) # 14 min 23 sec

##############################################################

# sample path
alpha = 0.1
decision_stat <- numeric(100)
g <- 0
t <- 1
num <- nrow(X_1)
while (t <= 50) {
  datum_t <- Y_simulated[t+50, ]
  tmp_dist <- numeric(num)
  for (j in 1:num) {
    datum_j <- X_1[j, ]
    dist_tj <- dist2(datum_t, datum_j)
    tmp_dist[j] <- dist_tj
  }
  sort_dist <- sort(tmp_dist)
  sum_kNN <- sum(sort_dist[1:k])
  tail_prob <- sum(baseline_distances > sum_kNN) / (N - M)
  if (tail_prob == 0) { # can do some modifications?
    tail_prob <- 1 / (N - M)
  }
  g <- g + log(alpha / tail_prob)
  if (g < 0) {
    g <- 0
  }
  decision_stat[t] <- g
  t <- t + 1
}

decision_stat_df <- data.frame(time = 1:100, decision_stat = decision_stat)
ggplot(decision_stat_df, aes(x = time, y = decision_stat)) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 25, linetype = "dashed", color = "red") +
  labs(x = expression(t), y = expression(g[t])) +
  theme_minimal() +
  ylim(0, 5) +
  theme(text = element_text(size = 14))

##############################################################

performance100 <- function(result){
  cp <- readRDS(result)
  print(paste0("Successful detection count: ", length(cp[which((cp+75) >= 100)]), " Average delay: ", round(mean(cp[which((cp+75) >= 100)]+75-100), 3), " Undetected change count: ", 100 - length(cp)))
}
performance150 <- function(result){
  cp <- readRDS(result)
  print(paste0("Successful detection count: ", length(cp[which((cp+75) >= 150)]), " Average delay: ", round(mean(cp[which((cp+75) >= 150)]+75-150), 3), " Undetected change count: ", 100 - length(cp)))
}
```

# Verify ARLs
```{r}
# Analytical
start.time <- Sys.time()

h_value <- log(500/10.1)/(1-lambertWp(0.2*log(0.2))/log(0.2))
n = 75
Y_simulated = sim1(rho=0.2, mean_change=FALSE, scale=1, cov_change=FALSE)
baseline_distances <- baseline(Y_simulated)
avg_RL(h_value, Y_simulated, baseline_distances) # get ARL

end.time <- Sys.time()
print(end.time - start.time)

# Empirical
# start.time <- Sys.time()
# 
# n = 200
# Y_simulated = sim1(rho=0.2, mean_change=FALSE, scale=1, cov_change=FALSE)
# baseline_distances <- baseline(Y_simulated)
# h <- seq(2.5, 6.5, by = 0.05)
# mean_fap <- avg_RL(h, Y_simulated, baseline_distances)
# h_value <- h[which(mean_fap > 500)[1]]
# avg_RL(h_value, Y_simulated, baseline_distances) # get ARL
# 
# end.time <- Sys.time()
# print(end.time - start.time)
```

# Mean change

```{r}
start.time <- Sys.time()
count = 0; cp <- c()
for(i in 1:100){
  # print(paste0("Run: ", i))
  n = 150 # 200
  Y_simulated = sim1(rho=0.2, mean_change=TRUE, scale=1, cov_change=FALSE)
  baseline_distances <- baseline(Y_simulated)
  # h <- seq(0.01, 5, by = 0.02)
  # mean_fap <- avg_RL(h, Y_simulated, baseline_distances)
  # h_value <- h[which(mean_fap > 500)[1]]
  h_value <- log(1500/10.1)/(1-lambertWp(0.2*log(0.2))/log(0.2))
  # print(h_value) # get threshold values
  t <- 1; g <- 0; N <- 75; M <- 25; k <- 4; alpha = 0.2
  X_1 = Y_simulated[1:M, ]
  # agg <- 0
  while(g < h_value){ # agg < 5 && t+50 < 101
    if(t > (150-N)){ # 200-N
      break
    }
    datum_t <- Y_simulated[t+N, ]
    tmp_dist <- numeric(M)
    for (j in 1:M) {
      datum_j <- X_1[j, ]
      dist_tj <- dist2(datum_t, datum_j, t_values)
      tmp_dist[j] <- dist_tj
    }
    sort_dist <- sort(tmp_dist)
    sum_kNN <- sum(sort_dist[1:k])
    # all <- rbind(Y_simulated[1:50, ], datum_t) 
    tail_prob <- sum(baseline_distances > sum_kNN) / (N - M)
    # F <- ecdf(total_variation_depth(all)$tvd)
    # tail_prob <- F(total_variation_depth(all)$tvd[51])
    if (tail_prob == 0) { # could do some modifications?
      tail_prob <- 1 / (N - M)
    }
    # if (tail_prob < 0.05){
    #   agg = agg + 1
    # }
    g <- g + log(alpha / tail_prob)
    if (g < 0) {
      g <- 0
    }
    t <- t + 1
  }
  cp <- c(cp, t)
  if(t >= 25){ # 75
    count = count + 1
  }
}
print(count)
saveRDS(cp, "Realtime_mean_1500arl_cp100.rds")
end.time <- Sys.time()
print(end.time - start.time)

cp <- readRDS("Realtime_mean_500arl.rds") # 7.622535 mins
print(paste0("Successful detection count: ", length(cp[which((cp+50) >= 75 & (cp+50) <= 90)])))
print(paste0("Average delay: ", mean(cp[which((cp+50) >= 75)]-25)))
print(paste0("Undetected change count: ", 100 - length(cp)))

performance150("Realtime_mean_500arl_cp150.rds")
performance150("Realtime_mean_1000arl_cp150.rds")
performance150("Realtime_mean_1500arl_cp150.rds")
performance100("Realtime_mean_500arl_cp100.rds")
performance100("Realtime_mean_1000arl_cp100.rds")
performance100("Realtime_mean_1500arl_cp100.rds")
```

# Scale change

```{r}
start.time <- Sys.time()
count = 0; cp <- c()
for(i in 1:100){
  # print(paste0("Run: ", i))
  n = 150 # 200
  Y_simulated = sim1(rho=0.2, mean_change=FALSE, scale=2, cov_change=FALSE)
  baseline_distances <- baseline(Y_simulated)
  # h <- seq(0.01, 5, by = 0.02)
  # mean_fap <- avg_RL(h, Y_simulated, baseline_distances)
  # h_value <- h[which(mean_fap > 500)[1]]
  h_value <- log(1500/10.1)/(1-lambertWp(0.2*log(0.2))/log(0.2))
  t <- 1; g <- 0; N <- 75; M <- 25; k <- 4; alpha = 0.2
  X_1 = Y_simulated[1:M, ]
  while(g < h_value){
    if(t > (150-N)){ # 200-N
      break
    }
    datum_t <- Y_simulated[t+N, ]
    tmp_dist <- numeric(M)
    for (j in 1:M) {
      datum_j <- X_1[j, ]
      dist_tj <- dist2(datum_t, datum_j, t_values)
      tmp_dist[j] <- dist_tj
    }
    sort_dist <- sort(tmp_dist)
    sum_kNN <- sum(sort_dist[1:k])
    tail_prob <- sum(baseline_distances > sum_kNN) / (N - M)
    if (tail_prob == 0) {
      tail_prob <- 1 / (N - M)
    }
    g <- g + log(alpha / tail_prob)
    if (g < 0) {
      g <- 0
    }
    t <- t + 1
  }
  cp <- c(cp, t)
  if(t >= 25){ # 75
    count = count + 1
  }
}
print(count)
saveRDS(cp, "Realtime_scale_1500arl_cp100.rds")
end.time <- Sys.time()
print(end.time - start.time)

# cp <- readRDS("Realtime_scale_500arl.rds") # 8.549889 mins
# print(paste0("Successful detection count: ", length(cp[which((cp+50) >= 75 & (cp+50) <= 90)])))
# print(paste0("Average delay: ", mean(cp[which((cp+50) >= 75)]-25)))
# print(paste0("Undetected change count: ", 100 - length(cp)))

performance150("Realtime_scale_500arl_cp150.rds")
performance150("Realtime_scale_1000arl_cp150.rds")
performance150("Realtime_scale_1500arl_cp150.rds")
performance100("Realtime_scale_500arl_cp100.rds")
performance100("Realtime_scale_1000arl_cp100.rds")
performance100("Realtime_scale_1500arl_cp100.rds")
```

# Mean + Scale change

```{r}
start.time <- Sys.time()
count = 0; cp <- c()
for(i in 1:100){
  # print(paste0("Run: ", i))
  n = 150 # 200
  Y_simulated = sim1(rho=0.2, mean_change=TRUE, scale=2, cov_change=FALSE)
  baseline_distances <- baseline(Y_simulated)
  # h <- seq(0.01, 5, by = 0.02)
  # mean_fap <- avg_RL(h, Y_simulated, baseline_distances)
  # h_value <- h[which(mean_fap > 500)[1]]
  h_value <- log(500/10.1)/(1-lambertWp(0.2*log(0.2))/log(0.2))
  t <- 1; g <- 0; N <- 75; M <- 25; k <- 4; alpha = 0.2
  X_1 = Y_simulated[1:M, ]
  while(g < h_value){
    if(t > (150-N)){ # 200-N
      break
    }
    datum_t <- Y_simulated[t+N, ]
    tmp_dist <- numeric(M)
    for (j in 1:M) {
      datum_j <- X_1[j, ]
      dist_tj <- dist2(datum_t, datum_j, t_values)
      tmp_dist[j] <- dist_tj
    }
    sort_dist <- sort(tmp_dist)
    sum_kNN <- sum(sort_dist[1:k])
    tail_prob <- sum(baseline_distances > sum_kNN) / (N - M)
    if (tail_prob == 0) {
      tail_prob <- 1 / (N - M)
    }
    g <- g + log(alpha / tail_prob)
    if (g < 0) {
      g <- 0
    }
    t <- t + 1
  }
  cp <- c(cp, t)
  if(t >= 25){ # 75
    count = count + 1
  }
}
print(count)
saveRDS(cp, "Realtime_mean_scale_500arl_cp100.rds")
end.time <- Sys.time()
print(end.time - start.time)

# cp <- readRDS("Realtime_mean_scale_500arl.rds") # 8.823159 mins
# print(paste0("Successful detection count: ", length(cp[which((cp+50) >= 75 & (cp+50) <= 90)])))
# print(paste0("Average delay: ", mean(cp[which((cp+50) >= 75)]-25)))
# print(paste0("Undetected change count: ", 100 - length(cp)))

performance150("Realtime_mean_scale_500arl_cp150.rds")
performance150("Realtime_mean_scale_1000arl_cp150.rds")
performance150("Realtime_mean_scale_1500arl_cp150.rds")
performance100("Realtime_mean_scale_500arl_cp100.rds")
performance100("Realtime_mean_scale_1000arl_cp100.rds")
performance100("Realtime_mean_scale_1500arl_cp100.rds")
```

# Covariance structure change

```{r}
start.time <- Sys.time()
count = 0; cp <- c()
for(i in 1:100){
  # print(paste0("Run: ", i))
  n = 150 # 200
  Y_simulated = sim1(rho=0.2, mean_change=FALSE, scale=1, cov_change=TRUE)
  baseline_distances <- baseline(Y_simulated)
  # h <- seq(0.01, 5, by = 0.02)
  # mean_fap <- avg_RL(h, Y_simulated, baseline_distances)
  # h_value <- h[which(mean_fap > 500)[1]]
  h_value <- log(500/10.1)/(1-lambertWp(0.2*log(0.2))/log(0.2))
  t <- 1; g <- 0; N <- 75; M <- 25; k <- 4; alpha = 0.2
  X_1 = Y_simulated[1:M, ]
  while(g < h_value){
    if(t > (150-N)){ # 200-N
      break
    }
    datum_t <- Y_simulated[t+N, ]
    tmp_dist <- numeric(M)
    for (j in 1:M) {
      datum_j <- X_1[j, ]
      dist_tj <- dist2(datum_t, datum_j, t_values)
      tmp_dist[j] <- dist_tj
    }
    sort_dist <- sort(tmp_dist)
    sum_kNN <- sum(sort_dist[1:k])
    tail_prob <- sum(baseline_distances > sum_kNN) / (N - M)
    if (tail_prob == 0) {
      tail_prob <- 1 / (N - M)
    }
    g <- g + log(alpha / tail_prob)
    if (g < 0) {
      g <- 0
    }
    t <- t + 1
  }
  cp <- c(cp, t)
  if(t >= 25){ # 75
    count = count + 1
  }
}
print(count)
saveRDS(cp, "Realtime_cov_500arl_cp100.rds")
end.time <- Sys.time()
print(end.time - start.time)

# cp <- readRDS("Realtime_cov_500arl.rds") # 8.571249 mins
# print(paste0("Successful detection count: ", length(cp[which((cp+50) >= 75 & (cp+50) <= 90)])))
# print(paste0("Average delay: ", mean(cp[which((cp+50) >= 75)]-25)))
# print(paste0("Undetected change count: ", 100 - length(cp)))

performance150("Realtime_cov_500arl_cp150.rds")
performance150("Realtime_cov_1000arl_cp150.rds")
performance150("Realtime_cov_1500arl_cp150.rds")
performance100("Realtime_cov_500arl_cp100.rds")
performance100("Realtime_cov_1000arl_cp100.rds")
performance100("Realtime_cov_1500arl_cp100.rds")
```
